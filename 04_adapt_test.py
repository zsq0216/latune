import argparse
import time
import multiprocessing
import sys
import psutil
from threading import Thread, Event
from pynvml import *
import torch
import signal

class SystemLoadGenerator:
    def __init__(self, memory_mb=0, cpu_cores=0, gpu_calc_intensity=0, gpu_mem_intensity=0):
        self.memory_mb = memory_mb
        self.cpu_cores = cpu_cores
        self.gpu_calc_intensity = gpu_calc_intensity or 0
        self.gpu_mem_intensity = gpu_mem_intensity or 0

        self.stop_event = Event()
        self.processes = []
        self.threads = []
        self.global_memory = []

    def memory_load(self, mb):
        bytes_amount = mb * 1024 * 1024
        return bytearray(bytes_amount)

    def cpu_worker(self, stop_event):
        while not stop_event.is_set():
            sum(i * i for i in range(10**7))

    def gpu_worker(self, intensity, mem_gb):
        try:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            chunk_size = 2**20
            memory_blocks = []

            mem_bytes = min(int(12 * 0.9 * 1024**3), mem_gb * 1024**3)
            for _ in range(0, mem_bytes, chunk_size):
                block = torch.zeros(chunk_size // 4, dtype=torch.float32, device=device)
                memory_blocks.append(block)

            size = 500 * intensity
            a = torch.randn(size, size, device=device)
            b = torch.randn(size, size, device=device)

            while not self.stop_event.is_set():
                c = torch.mm(a, b)
                a = c * 0.9 + torch.randn_like(c) * 0.1
                b = c.T * 0.9 + torch.randn_like(c.T) * 0.1
                torch.cuda.synchronize()

        except ImportError:
            print("éœ€è¦å®‰è£…PyTorch: pip install torch")
        except Exception as e:
            print(f"GPUé”™è¯¯: {e}")

    def get_system_stats(self):
        cpu_percent = psutil.cpu_percent(interval=0.1)
        sys_mem = psutil.virtual_memory()
        mem_used = sys_mem.used / (1024 ** 3)
        mem_total = sys_mem.total / (1024 ** 3)

        gpu_info = []
        try:
            nvmlInit()
            for i in range(nvmlDeviceGetCount()):
                handle = nvmlDeviceGetHandleByIndex(i)
                util = nvmlDeviceGetUtilizationRates(handle)
                mem = nvmlDeviceGetMemoryInfo(handle)
                gpu_info.append({
                    "gpu_util": util.gpu,
                    "mem_util": mem.used / mem.total * 100,
                    "mem_used": mem.used / (1024 ** 3),
                    "mem_total": mem.total / (1024 ** 3)
                })
            nvmlShutdown()
        except NVMLError:
            pass

        return {
            "cpu": cpu_percent,
            "memory": f"{mem_used:.1f}/{mem_total:.1f}GB ({sys_mem.percent}%)",
            "gpu": gpu_info
        }

    def get_available_resources(self):
        """
        è¿”å›å½“å‰ç³»ç»Ÿçš„å¯ç”¨èµ„æºä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - å¯ç”¨å†…å­˜ï¼ˆMBï¼‰
        - CPU ç©ºé—²ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰
        - æ¯å¼  GPU çš„å‰©ä½™æ˜¾å­˜ï¼ˆMBï¼‰
        """
        # è·å–å†…å­˜ä¿¡æ¯
        mem = psutil.virtual_memory()
        mem_available_mb = mem.available / 1024 / 1024  # MB

        # è·å– CPU ç©ºé—²ç‡ï¼ˆ100 - å½“å‰ä½¿ç”¨ç‡ï¼‰
        cpu_idle_percent = 100 - psutil.cpu_percent(interval=0.1)

        # è·å– GPU å¯ç”¨æ˜¾å­˜
        gpu_free_mem_list = []
        try:
            nvmlInit()
            for i in range(nvmlDeviceGetCount()):
                handle = nvmlDeviceGetHandleByIndex(i)
                mem_info = nvmlDeviceGetMemoryInfo(handle)
                free_mem_mb = mem_info.free / 1024 / 1024  # MB
                gpu_free_mem_list.append(free_mem_mb)
            nvmlShutdown()
        except NVMLError:
            gpu_free_mem_list = []

        return {
            "mem_avail": round(mem_available_mb*0.9, 2),
            "cpu_avail": round(cpu_idle_percent*0.9, 2),
            "gpu_avail": gpu_free_mem_list[0]*0.9
        }

    def monitor(self):
        try:
            while not self.stop_event.wait(1):
                stats = self.get_system_stats()
                print("\033c", end="")

                print("=== å®æ—¶ç³»ç»Ÿç›‘æ§ ===")
                print(f"CPUä½¿ç”¨ç‡: {stats['cpu']}%")
                print(f"å†…å­˜ä½¿ç”¨: {stats['memory']}")
                if stats['gpu']:
                    for i, gpu in enumerate(stats['gpu']):
                        print(f"GPU{i} ä½¿ç”¨ç‡: {gpu['gpu_util']}% | æ˜¾å­˜: {gpu['mem_used']:.1f}/{gpu['mem_total']:.1f}GB ({gpu['mem_util']:.1f}%)")
                else:
                    print("GPUä¿¡æ¯: ä¸å¯ç”¨")
                print("\næŒ‰ Ctrl+C é€€å‡º")
        except KeyboardInterrupt:
            self.stop_event.set()

    def _collect_resource_samples(self, resource_type='gpu', duration=10, interval=1.0):
        """
        é‡‡é›†èµ„æºæ ·æœ¬ï¼Œè¿”å› (èµ„æºå¹³å‡å€¼, å†…å­˜å¹³å‡å€¼)
        """
        resource_samples = []
        memory_samples = []

        for _ in range(int(duration / interval)):
            stats = self.get_system_stats()
            
            if resource_type == 'cpu':
                resource_val = stats['cpu']
            elif resource_type == 'gpu':
                if stats['gpu']:
                    resource_val = max(g['mem_used'] for g in stats['gpu'])
                else:
                    resource_val = 0
            else:
                raise ValueError("èµ„æºç±»å‹å¿…é¡»æ˜¯ 'cpu' æˆ– 'gpu'")

            # ç»Ÿä¸€è·å–å†…å­˜ä½¿ç”¨ç‡
            mem_percent = float(stats['memory'].split('(')[-1].strip('% )'))
            resource_samples.append(resource_val)
            memory_samples.append(mem_percent)
            time.sleep(interval)

        avg_resource = sum(resource_samples) / len(resource_samples)
        avg_memory = sum(memory_samples) / len(memory_samples)
        return avg_resource, avg_memory

    def detect_fluctuation_continuous(self, resource_type='gpu', duration=10, interval=10, threshold=0.1, notify_func=None):
        """
        æŒç»­è¿è¡Œçš„èµ„æºæ³¢åŠ¨æ£€æµ‹å™¨ã€‚
        æ¯ interval ç§’é‡‡æ ·ä¸€æ¬¡èµ„æºï¼Œé‡‡æ · duration ç§’å¹¶å¯¹æ¯”å‰ä¸€æ¬¡çš„å¹³å‡å€¼ï¼›
        è‹¥ CPU/GPU ä¸ Memory ä½¿ç”¨ç‡åŒæ—¶å˜åŒ–è¶…è¿‡ threshold%ï¼Œåˆ™è§¦å‘é€šçŸ¥ã€‚

        :param resource_type: 'cpu' æˆ– 'gpu'
        :param duration: æ¯æ¬¡é‡‡æ ·æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        :param interval: æ¯ interval ç§’å¯¹æ¯”ä¸€æ¬¡ï¼ˆå®é™…é‡‡æ ·å‘¨æœŸ = interval + durationï¼‰
        :param threshold: å¹³å‡å€¼å˜åŠ¨é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
        :param notify_func: å›è°ƒå‡½æ•°ï¼Œå‚æ•° (changed: bool, message: str)
        """
        def monitor_loop():
            prev_resource, prev_memory = self._collect_resource_samples(resource_type, duration)

            while not self.stop_event.is_set():
                time.sleep(interval)
                curr_resource, curr_memory = self._collect_resource_samples(resource_type, duration)

                delta_resource = abs(curr_resource - prev_resource)
                delta_memory = abs(curr_memory - prev_memory)

                resource_changed = delta_resource/prev_memory >= threshold
                memory_changed = delta_memory/prev_memory >= threshold

                print("prev_resource:", prev_resource, "curr_resource:", curr_resource)
                print("prev_memory:", prev_memory, "curr_memory:", curr_memory)
                print("resource_changed:", resource_changed, "memory_changed:", memory_changed)

                if resource_changed and memory_changed:
                    message = (f"âš ï¸ èµ„æºå˜åŠ¨æ£€æµ‹:\n"
                               f"  {resource_type.upper()}: {prev_resource:.1f}% â†’ {curr_resource:.1f}% "
                               f"(Î”{delta_resource:.1f}%)\n"
                               f"  å†…å­˜: {prev_memory:.1f}% â†’ {curr_memory:.1f}% "
                               f"(Î”{delta_memory:.1f}%)")
                    if notify_func:
                        notify_func(True, message)
                    else:
                        print(message)
                else:
                    if notify_func:
                        notify_func(False, "æ— æ˜¾è‘—å˜åŠ¨")
                    else:
                        print("âœ… æ— æ˜¾è‘—èµ„æºå˜åŠ¨")

                prev_resource, prev_memory = curr_resource, curr_memory

        fluctuation_thread = Thread(target=monitor_loop)
        fluctuation_thread.daemon = True
        fluctuation_thread.start()

    def run(self):
        def handle_exit(signum, frame):
            print(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œè§¦å‘æ¸…ç†")
            self.cleanup()
            sys.exit(0)  # ç¡®ä¿é€€å‡ºæ•´ä¸ªç¨‹åº

        signal.signal(signal.SIGINT, handle_exit)   # Ctrl+C
        signal.signal(signal.SIGTERM, handle_exit)  # kill -15 pid
        
        monitor_thread = Thread(target=self.monitor)
        monitor_thread.start()

        try:
            if self.memory_mb:
                self.global_memory.append(self.memory_load(self.memory_mb))
                print(f"âœ… å·²åˆ†é… {self.memory_mb}MB å†…å­˜")

            if self.cpu_cores:
                for _ in range(self.cpu_cores):
                    p = multiprocessing.Process(target=self.cpu_worker, args=(self.stop_event,))
                    p.start()
                    self.processes.append(p)
                print(f"âœ… å·²å¯åŠ¨ {self.cpu_cores} ä¸ªCPUè´Ÿè½½è¿›ç¨‹")

            if self.gpu_calc_intensity or self.gpu_mem_intensity:
                MEMORY_LEVELS = {
                    1: 1,  2: 2,  3: 3,  4: 4,  5: 6,
                    6: 8,  7: 9,  8: 10, 9: 11, 10: 10.8
                }
                mem_gb = MEMORY_LEVELS.get(self.gpu_mem_intensity, 0)
                t = Thread(target=self.gpu_worker, args=(self.gpu_calc_intensity, mem_gb))
                t.daemon = True
                t.start()
                self.threads.append(t)
                print(f"âœ… å·²å¯åŠ¨GPUè´Ÿè½½ [è®¡ç®—:{self.gpu_calc_intensity}/æ˜¾å­˜:{mem_gb}GB]")

            while not self.stop_event.is_set():
                time.sleep(0.5)

        except KeyboardInterrupt:
            self.cleanup()
            sys.exit(0)

    def cleanup(self):
        print("\nğŸ›‘ æ­£åœ¨æ¸…ç†èµ„æº...")
        self.stop_event.set()

        for p in self.processes:
            p.terminate()

        for t in self.threads:
            t.join(timeout=2)

        self.global_memory.clear()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        import gc
        gc.collect()

        print("âœ… æ¸…ç†å®Œæˆ")

# å‘½ä»¤è¡Œå…¥å£
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç³»ç»Ÿè´Ÿè½½ç”Ÿæˆå™¨V2ï¼ˆç±»å°è£…ç‰ˆï¼‰")
    parser.add_argument('--memory', type=int, help='å†…å­˜è´Ÿè½½å¤§å°ï¼ˆMBï¼‰')
    parser.add_argument('--cpu', type=int, help='CPUè´Ÿè½½æ ¸æ•°')
    parser.add_argument('--gpu-calc', type=int, help='GPUè®¡ç®—å¼ºåº¦ï¼ˆ1-10ï¼‰')
    parser.add_argument('--gpu-mem', type=int, help='GPUæ˜¾å­˜å¼ºåº¦ï¼ˆ1-10ï¼‰')
    args = parser.parse_args()

    generator = SystemLoadGenerator(
        memory_mb=args.memory or 0,
        cpu_cores=args.cpu or 0,
        gpu_calc_intensity=args.gpu_calc or 0,
        gpu_mem_intensity=args.gpu_mem or 0
    )
    generator.run()



